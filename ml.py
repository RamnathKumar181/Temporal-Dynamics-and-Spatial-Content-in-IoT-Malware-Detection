import numpy as np
import pandas as pd


# Importing the Keras libraries and packages
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Dropout

from sklearn.metrics import cohen_kappa_score
from keras.callbacks import ReduceLROnPlateau
from keras.callbacks import ModelCheckpoint
from keras import optimizers
import matplotlib.pyplot as plt

A_train =  np.load('NPY/train_data_1.npy', encoding = 'latin1') # MODEL  1
A_valid = np.load('NPY/test_data_1.npy', encoding = 'latin1')

from sklearn.cross_validation import train_test_split
A,B = train_test_split(A_valid,test_size=0.5,random_state=0)

training_data=[]
for i in A_train:
    training_data.append(i)
for i in  A:
    training_data.append(i)
test_data=[]
for i in B:
    test_data.append(i)

X_train = np.array([i[0] for i in training_data])
y_train = np.array([i[1] for i in training_data])

X_test = np.array([i[0] for i in test_data])
y_test = np.array([i[1] for i in test_data])


from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X_train = sc_X.fit_transform(X_train)

X_test = sc_X.transform(X_test)


mal=0
ben=0
for i in y_train:
    if i==0:
        ben=ben+1
    else:
        mal=mal+1

print(mal)
print(ben)
mal=0
ben=0
for i in y_test:
    if i==0:
        ben=ben+1
    else:
        mal=mal+1
print(mal)
print(ben)
print(X_train.shape)

X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))


# Initialising the RNN
regressor = Sequential()

# Adding the first LSTM layer and some Dropout regularisation
regressor.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))
regressor.add(Dropout(0.2))

# Adding a second LSTM layer and some Dropout regularisation
regressor.add(LSTM(units = 50, return_sequences = True))
regressor.add(Dropout(0.2))

# Adding a third LSTM layer and some Dropout regularisation
regressor.add(LSTM(units = 25, return_sequences = True))
regressor.add(Dropout(0.2))

# Adding a fourth LSTM layer and some Dropout regularisation
regressor.add(LSTM(units = 25))
regressor.add(Dropout(0.2))

# Adding the output layer
regressor.add(Dense(units = 1,activation='sigmoid'))

adam = optimizers.Adam(lr=5e-7,beta_1=0.9,beta_2=0.999)

# Compiling the RNN
regressor.compile(optimizer = adam, loss = 'binary_crossentropy', metrics = ['accuracy'])
# regressor.load_weights('output.hdf5')

# # Fitting the RNN to the Training set
checkpoint = ModelCheckpoint('output.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='max')
reducelr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose= 1, mode = 'auto')
# # 0.001 for 100 epochs, 0.8 reducelr,batch 32
# #0.0001 for 30 epochs, 0.5 reducelr,batch 16
history = regressor.fit(x=X_train,y=y_train,batch_size=32, validation_data=( X_test, y_test),epochs=9,callbacks = [reducelr, checkpoint])
